{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO2BWFsaZqGF8Vpc+iZ5WxH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmkarakaya/Deep-Learning-Tutorials/blob/master/Simple_Rag_with_chromaDB_Gemini_PartA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will develop a Retrieval Augmented Generation (RAG) application.\n",
        "\n",
        "The Parts are\n",
        "\n",
        "* PART A: AN INTRO TO GEMINI API FOR TEXT GENERATION & CHAT\n",
        "* PART B: AN INTRO TO CHROMADB FOR VECTOR STORAGE & SIMILARITY SEARCH\n",
        "* PART C: A SIMPLE RAG BASED ON GEMINI & CHROMADB"
      ],
      "metadata": {
        "id": "HGmLFS7j8JSQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Watch this notebook on the Murat Karakaya Akademi YouTube channel:\n",
        "* In English: https://www.youtube.com/playlist?list=PLQflnv_s49v-EFKdOVDKB743f1iskLLw2\n",
        "* In Turkish: https://www.youtube.com/playlist?list=PLQflnv_s49v_nrk7iGYqw5iRAKrSZPnnV"
      ],
      "metadata": {
        "id": "DA9q6TTE-g4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART A: AN INTRO TO GEMINI API FOR TEXT GENERATION & CHAT\n",
        "\n",
        "\n",
        "RAG stands for Retrieval-Augmented Generation. It's a technique that combines large language models (LLMs) with external knowledge sources to improve the accuracy and reliability of AI-generated text.\n",
        "\n",
        "## How Does RAG Work? Unveiling the Power of External Knowledge\n",
        "\n",
        "Before we start the core RAG process, we need to provide a foundation as follows:\n",
        "\n",
        "* **Building the Knowledge Base:** The system starts by transforming documents and information within the external knowledge base (like Wikipedia or a company database) into a special format called **vector representations**. These condense the meaning of each document into a series of **numbers**, capturing the essence of the content.\n",
        "\n",
        "* **Vector Database for Speedy Retrieval**: These vector representations are then stored in a specialized database called a vector database. This database is optimized for efficiently **searching and retrieving** information based on **semantic similarity**. Imagine it as a super-powered library catalog that **understands the meaning** of documents, **not just keywords**.\n",
        "\n",
        "Now, let's explore how RAG leverages this foundation:\n",
        "\n",
        "* **User Input**: The RAG process begins with a question or **prompt** from the user. This could be anything from \"What caused the extinction of the dinosaurs?\" to a more open-ended request like \"Write a creative story.\"\n",
        "\n",
        "* **Intelligent Retrieval**: RAG doesn't rely solely on the **LLM's internal knowledge**. It employs an information retrieval component that acts like a super-powered search engine. This component scans the vast external knowledge base – like a company's internal database for specific domains – to find information **directly relevant** to the user's input. Unlike a traditional **search engine** that relies on **keywords**, RAG leverages the power of vector representations to understand the **semantic meaning** of the user's prompt and identify the most relevant documents.\n",
        "\n",
        "* **Enriched Context Creation**: The retrieved information isn't just shown alongside the prompt. RAG cleverly **merges the user input with the relevant snippets** from the knowledge base. This creates a ***richer context*** for the LLM to understand the **user's intent** and formulate a well-informed response.\n",
        "\n",
        "* **LLM Powered Response Generation**: Finally, the **enriched context** is fed to the Large Language Model (LLM). The LLM, along with its ability to process language patterns, now has a strong **foundation of factual** information to draw upon. This empowers it to generate a response that is both comprehensive and accurate, addressing the specific needs of the user's prompt.\n",
        "\n",
        "In this part, we will learn how provide an LLM connection and generate text using Google Gemini API.\n",
        "\n",
        "https://ai.google.dev/gemini-api/docs\n",
        "\n",
        "## CONTENT\n",
        "* The Python SDK for the Gemini API\n",
        "* Check the Google LLM Models available via the provided API\n",
        "* Interact with the models using 2 Alternative Interfaces\n",
        "  1. Generate text interface\n",
        "  2. Interact with the models using Multi-turn conversations (chat) interface\n",
        "\n",
        "* Understand Model & Chat objects\n",
        "  * Model Object in detail\n",
        "  * System Prompt in the Gemini API\n",
        "  * Chat Object in detail\n",
        "* Chat using system_instruction: ***A Manual RAG?***\n",
        "* How Many Tokens --> How much does it cost?\n",
        "* Build a simple Interface with Gradio\n"
      ],
      "metadata": {
        "id": "spB6iVzXxT1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the Python SDK\n",
        "\n",
        "* The Python SDK for the Gemini API, is contained in the [`google-generativeai`](https://pypi.org/project/google-generativeai/) package. Install the dependency using pip:"
      ],
      "metadata": {
        "id": "beMCPjuLLhAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "E607BT94Lb3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy as np\n",
        "#from tqdm import tqdm\n",
        "#import pathlib\n",
        "import os\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "7wZWDPDCJina"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The **to_markdown** function converts plain text from the LLM model to Markdown format, adding blockquote styling and converting bullet points."
      ],
      "metadata": {
        "id": "9R_7tVjp-hne"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SP-1LwIoGcGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ],
      "metadata": {
        "id": "Zfy7rLWNLt5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "k2rzs85R7iYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the Google LLM Models available via the provided API\n",
        "\n",
        "* You can see the names of the available models as follows:"
      ],
      "metadata": {
        "id": "9ztqhxys_OdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in genai.list_models():\n",
        "  if 'generateContent' in m.supported_generation_methods:\n",
        "    print(m.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "qU7wtu2e55z9",
        "outputId": "318901b6-7d15-405f-ab79-8f13432b9868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-pro\n",
            "models/gemini-pro-vision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* You can see the details of the models as follows:"
      ],
      "metadata": {
        "id": "i6gnd-fR_08c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [m for m in genai.list_models()]\n",
        "models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CbDIFnIA7oZ5",
        "outputId": "3e69ebdc-abd3-4ee9-8d92-1d647c98a479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Model(name='models/chat-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 Chat (Legacy)',\n",
              "       description='A legacy text-only model optimized for chat conversations',\n",
              "       input_token_limit=4096,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
              "       temperature=0.25,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/text-bison-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='PaLM 2 (Legacy)',\n",
              "       description='A legacy model that understands text and generates text as an output',\n",
              "       input_token_limit=8196,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "       temperature=0.7,\n",
              "       top_p=0.95,\n",
              "       top_k=40),\n",
              " Model(name='models/embedding-gecko-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding Gecko',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=1024,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedText', 'countTextTokens'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
              "                    'model that supports tuning.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Latest',\n",
              "       description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
              "                    'model.'),\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-1.0-pro-vision-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/gemini-1.5-flash',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash 001',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-flash-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Flash Latest',\n",
              "       description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro',\n",
              "       description='Mid-size multimodal model that supports up to 1 million tokens',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro 001',\n",
              "       description='Mid-size multimodal model that supports up to 1 million tokens',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-1.5-pro-latest',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.5 Pro Latest',\n",
              "       description='Mid-size multimodal model that supports up to 1 million tokens',\n",
              "       input_token_limit=1048576,\n",
              "       output_token_limit=8192,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=1.0,\n",
              "       top_p=0.95,\n",
              "       top_k=64),\n",
              " Model(name='models/gemini-pro',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro',\n",
              "       description='The best model for scaling across a wide range of tasks',\n",
              "       input_token_limit=30720,\n",
              "       output_token_limit=2048,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.9,\n",
              "       top_p=1.0,\n",
              "       top_k=None),\n",
              " Model(name='models/gemini-pro-vision',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Gemini 1.0 Pro Vision',\n",
              "       description='The best image understanding model to handle a broad range of applications',\n",
              "       input_token_limit=12288,\n",
              "       output_token_limit=4096,\n",
              "       supported_generation_methods=['generateContent', 'countTokens'],\n",
              "       temperature=0.4,\n",
              "       top_p=1.0,\n",
              "       top_k=32),\n",
              " Model(name='models/embedding-001',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Embedding 001',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/text-embedding-004',\n",
              "       base_model_id='',\n",
              "       version='004',\n",
              "       display_name='Text Embedding 004',\n",
              "       description='Obtain a distributed representation of a text.',\n",
              "       input_token_limit=2048,\n",
              "       output_token_limit=1,\n",
              "       supported_generation_methods=['embedContent'],\n",
              "       temperature=None,\n",
              "       top_p=None,\n",
              "       top_k=None),\n",
              " Model(name='models/aqa',\n",
              "       base_model_id='',\n",
              "       version='001',\n",
              "       display_name='Model that performs Attributed Question Answering.',\n",
              "       description=('Model trained to return answers to questions that are grounded in provided '\n",
              "                    'sources, along with estimating answerable probability.'),\n",
              "       input_token_limit=7168,\n",
              "       output_token_limit=1024,\n",
              "       supported_generation_methods=['generateAnswer'],\n",
              "       temperature=0.2,\n",
              "       top_p=1.0,\n",
              "       top_k=40)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interact with the models using 2 Alternative Interfaces\n",
        "\n",
        "1. Generate text\n",
        "2. Multi-turn conversations (chat)"
      ],
      "metadata": {
        "id": "IwvOrH0YR6mr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Generate text interface\n",
        "\n",
        "In the simplest case, you can pass a prompt string to the GenerativeModel.generate_content method:"
      ],
      "metadata": {
        "id": "hz2qZhHwSWfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "response = model.generate_content(\"How many different ways to acccess a model in Gemini API?\")\n",
        "to_markdown(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "CYOPquLVSjLM",
        "outputId": "e0a7202e-f423-43bc-92fe-2cef237286f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> It's not entirely clear what you mean by \"accessing a model\" in the Gemini API context. Could you please clarify your question? \n> \n> For example, do you mean:\n> \n> * **How many different API endpoints are there for interacting with Gemini models?**\n> * **How many different ways can I send a request to a Gemini model (e.g., using different programming languages, libraries, etc.)?**\n> * **How many different models are available in the Gemini API?**\n> \n> Once you provide more context, I can give you a more precise answer. \n"
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOoJBSzgUIGI",
        "outputId": "fe440978-e0e0-4869-dfb4-b12498cb7582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"text\": \"It's not entirely clear what you mean by \\\"accessing a model\\\" in the Gemini API context. Could you please clarify your question? \\n\\nFor example, do you mean:\\n\\n* **How many different API endpoints are there for interacting with Gemini models?**\\n* **How many different ways can I send a request to a Gemini model (e.g., using different programming languages, libraries, etc.)?**\\n* **How many different models are available in the Gemini API?**\\n\\nOnce you provide more context, I can give you a more precise answer. \\n\"\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"index\": 0,\n",
              "          \"safety_ratings\": [\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            },\n",
              "            {\n",
              "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
              "              \"probability\": \"NEGLIGIBLE\"\n",
              "            }\n",
              "          ]\n",
              "        }\n",
              "      ]\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"Which API did I ask you?\")\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "4UQhtWIsUDEX",
        "outputId": "2f5bafc6-d645-4940-a0e6-ca389ff67a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Please provide me with the context of our conversation so I can help you determine which API you're referring to. \n> \n> For example, tell me:\n> \n> * What were we discussing before?\n> * What were you trying to accomplish? \n> * What keywords did you use?\n> \n> The more information you provide, the better I can understand your question and assist you. \n"
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Interact with the models using Multi-turn conversations (chat) interface\n",
        "\n",
        "* This code snippet initializes a Gemini AI model and starts a chat session  with an empty conversation history."
      ],
      "metadata": {
        "id": "VBaNTOOF_6xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "#response = model.generate_content(\"How many different ways to acccess a model in the Gemini API?\")\n",
        "chat = model.start_chat(history=[])\n",
        "response = chat.send_message(\"How many different ways to acccess a model in the Gemini API?\")\n",
        "to_markdown(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "uE4sH55ABY-e",
        "outputId": "26311758-3aff-4191-f98b-d9ed8b663b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> I can't give you an exact number of ways to access a Gemini model through the API. This is because the specifics of the API and its access methods are proprietary information held by Google, and they may change over time. \n> \n> However, I can offer you some general information about the ways you might interact with a Gemini model through an API:\n> \n> **Typical API Access Methods**\n> \n> * **REST API:** This is the most common way to interact with APIs. You send requests to specific endpoints with data and parameters, and the API responds with data in a structured format (e.g., JSON). \n> * **gRPC:** A more efficient and high-performance communication protocol compared to REST APIs. This method is often used for large-scale or real-time applications.\n> * **SDKs:** Libraries that provide a more convenient way to interact with an API from within your chosen programming language. They usually abstract away the underlying communication details. \n> \n> **What to Look for**\n> \n> When you're looking for information on accessing a Gemini model through an API, keep these points in mind:\n> \n> * **Official documentation:** Google will likely provide documentation for their Gemini API, detailing the available endpoints, parameters, and examples.\n> * **Community resources:** Look for forums, blogs, and other resources where developers share their experiences and knowledge about accessing Gemini models.\n> * **Updates and announcements:** Stay updated on any official announcements or news from Google regarding the Gemini API, as methods and features might evolve.\n> \n> **Remember:** Google's API access and models are evolving. The best way to get the most accurate and up-to-date information is to consult Google's official documentation and announcements. \n"
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#response = model.generate_content(\"Which API did I ask you?\")\n",
        "response =chat.send_message(\"Which API did I ask you?\")\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "Kkt7RKG6VR8t",
        "outputId": "b999172d-7501-4eee-e119-f1a9a6b7297b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> You didn't actually ask me about a specific API.  You asked me how many ways there are to access a Gemini model through the API. You didn't specify which API you were referring to. \n> \n> If you have a particular API in mind, please let me know and I'll do my best to answer your question. \n"
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understand Model & Chat objects"
      ],
      "metadata": {
        "id": "q1Z667a7uEi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's check the created **model** object first, and then the **chat** object:"
      ],
      "metadata": {
        "id": "u7jTl8E3I1kW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9iGxcLJGE0A",
        "outputId": "383ef460-7c97-48a9-ed1d-7942c94fdd3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "genai.GenerativeModel(\n",
              "    model_name='models/gemini-1.5-flash-latest',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=None,\n",
              "    system_instruction=None,\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* genai.GenerativeModel(...): This creates a Gemini model object for interacting with the API.\n",
        "\n",
        "* model_name='models/gemini-1.5-flash-latest': This specifies which Gemini model version to use. Here, it's \"gemini-1.5-flash-latest\", a powerful model known for its capabilities.\n",
        "* generation_config={}: This is a dictionary for customizing how the model generates text. The empty braces {} mean you're using default generation settings.\n",
        "* safety_settings={}: This is for configuring safety features, like preventing harmful or inappropriate responses. Empty braces again mean you're using default settings.\n",
        "* tools=None: This part is for integrating external tools with the model (e.g., accessing information from a database). Since it's None, no external tools are being used.\n",
        "* **system_instruction=None:** This is similar to a \"system prompt\" in other models, but Gemini API doesn't directly support system prompts. This instruction might have some influence on the model's behavior, but it's not a standard system prompt feature."
      ],
      "metadata": {
        "id": "0of0_YMEJDLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System Prompt in the Gemini API:\n",
        "\n",
        "**What is a System Prompt?**\n",
        "\n",
        "In models like ChatGPT, a system prompt is a special instruction provided at the start of a conversation. It helps define the persona, tone, or overall purpose of the model's responses.\n",
        "\n",
        "\n",
        "Unfortunately, the Gemini API **does not** offer a concept directly equivalent to a \"system prompt\" as found in other large language models like ChatGPT.\n",
        "\n",
        "\n",
        "**How Gemini API Works**\n",
        "\n",
        "The Gemini API functions differently. It prioritizes a task-oriented approach, focusing on generating responses based on specific instructions and context provided through its API calls.\n",
        "\n",
        "**Alternatives for Defining Behavior:**\n",
        "\n",
        "While a dedicated system prompt is absent, you can achieve similar effects through these methods:\n",
        "* Prompt Engineering: Craft your API requests with clear and concise instructions, including desired tone, format, or limitations.\n",
        "* Contextualization: Provide relevant information and examples within your API call to guide Gemini's responses.\n",
        "Model Variants: Gemini API offers various model sizes. Choosing a specific size might align with your desired behavior (e.g., a larger model for more comprehensive responses)"
      ],
      "metadata": {
        "id": "3CARVemnAz9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt= \"\"\" As an attentive and supportive academic assistant,\n",
        "           your task is to provide assistance based solely on the provided\n",
        "           excerpts. I will provide you the question and related text.\n",
        "           Answer the following questions, ensuring your responses\n",
        "           are derived exclusively from the provided partial texts.\n",
        "           If the answer cannot be found within the provided excerpts,\n",
        "           kindly respond with 'I don't know'.\n",
        "           After answering each question, please provide a detailed\n",
        "           explanation, breaking down the answer step by step and relating\n",
        "           it to the provided excerpts.\n",
        "           If you are ready, I will provide you the question and related text.\n",
        "        \"\"\""
      ],
      "metadata": {
        "id": "bptkaqzV6AkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-flash-latest', system_instruction=system_prompt)\n",
        "chat = model.start_chat(history=[])"
      ],
      "metadata": {
        "id": "UJ3uzuLkGgDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olt2Et5OGkr3",
        "outputId": "5655bfd5-6a0f-41a4-9797-0daa5c91734e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "genai.GenerativeModel(\n",
              "    model_name='models/gemini-1.5-flash-latest',\n",
              "    generation_config={},\n",
              "    safety_settings={},\n",
              "    tools=None,\n",
              "    system_instruction=\" As an attentive and supportive academic assistant,\\n           your task is to provide assistance based solely on the provided\\n           excerpts. I will provide you the question and related text.\\n           Answer the following questions, ensuring your responses\\n           are derived exclusively from the provided partial texts.\\n           If the answer cannot be found within the provided excerpts,\\n           kindly respond with 'I don't know'.\\n           After answering each question, please provide a detailed\\n           explanation, breaking down the answer step by step and relating\\n           it to the provided excerpts.\\n           If you are ready, I will provide you the question and related text.\\n        \",\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Does system_instruction work as system_prompt?\n",
        "\n",
        "Let's check:\n",
        "* This code snippet interacts with the Gemini chat session we initiated above.\n",
        "1. Sends your question/prompt to the Gemini chat.\n",
        "2. Times how long it takes Gemini to respond.\n",
        "3. Formats the Gemini's response into Markdown for cleaner display."
      ],
      "metadata": {
        "id": "aPUF-yNsCKcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"What is your task? \"\n",
        "response = chat.send_message(prompt)\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "qTTko3uuCDFW",
        "outputId": "5a7c458c-cdc1-4688-bf2d-7f39929fb702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> My task is to act as an attentive and supportive academic assistant. I will answer your questions based solely on the provided excerpts. \n> \n> Here's a breakdown:\n> \n> 1. **Provide Text:** You will give me a question and the relevant text excerpt.\n> 2. **Answer the Question:** I will answer your question using only information found within the provided text.\n> 3. **Detailed Explanation:**  I will explain my answer step-by-step, highlighting the relevant parts of the excerpt that support my response.\n> 4. **\"I Don't Know\":** If the answer cannot be found within the provided excerpts, I will honestly state \"I don't know.\"\n> \n> I am ready to assist you! Please provide me with your question and the related text. \n"
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Object in detail\n",
        "* Let's observe the **chat** object:"
      ],
      "metadata": {
        "id": "q6WJEfbnLaAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgYA8E78Fhon",
        "outputId": "ad5a4849-745f-4878-c098-85fe324c47c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatSession(\n",
              "    model=genai.GenerativeModel(\n",
              "        model_name='models/gemini-1.5-flash-latest',\n",
              "        generation_config={},\n",
              "        safety_settings={},\n",
              "        tools=None,\n",
              "        system_instruction=\" As an attentive and supportive academic assistant,\\n           your task is to provide assistance based solely on the provided\\n           excerpts. I will provide you the question and related text.\\n           Answer the following questions, ensuring your responses\\n           are derived exclusively from the provided partial texts.\\n           If the answer cannot be found within the provided excerpts,\\n           kindly respond with 'I don't know'.\\n           After answering each question, please provide a detailed\\n           explanation, breaking down the answer step by step and relating\\n           it to the provided excerpts.\\n           If you are ready, I will provide you the question and related text.\\n        \",\n",
              "    ),\n",
              "    history=[protos.Content({'parts': [{'text': 'What is your task? '}], 'role': 'user'}), protos.Content({'parts': [{'text': 'My task is t...ated text. \\n'}], 'role': 'model'})]\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can access the chat history:"
      ],
      "metadata": {
        "id": "F2jAKnuGLl4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLBFVENxZ2zh",
        "outputId": "3aca5ec3-0ca1-4494-e985-d0d88aab5eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[parts {\n",
              "   text: \"What is your task? \"\n",
              " }\n",
              " role: \"user\",\n",
              " parts {\n",
              "   text: \"My task is to act as an attentive and supportive academic assistant. I will answer your questions based solely on the provided excerpts. \\n\\nHere\\'s a breakdown:\\n\\n1. **Provide Text:** You will give me a question and the relevant text excerpt.\\n2. **Answer the Question:** I will answer your question using only information found within the provided text.\\n3. **Detailed Explanation:**  I will explain my answer step-by-step, highlighting the relevant parts of the excerpt that support my response.\\n4. **\\\"I Don\\'t Know\\\":** If the answer cannot be found within the provided excerpts, I will honestly state \\\"I don\\'t know.\\\"\\n\\nI am ready to assist you! Please provide me with your question and the related text. \\n\"\n",
              " }\n",
              " role: \"model\"]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's see the **chat history** in a bit formatted way:"
      ],
      "metadata": {
        "id": "m-u8Xi9EMWfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def printChatHistory():\n",
        "  for message in chat.history:\n",
        "    display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))\n",
        "    display('_'*80)\n"
      ],
      "metadata": {
        "id": "lOV63wmDX9V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "printChatHistory()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "TyqP2ix_PCoC",
        "outputId": "8ca0b208-14ae-437f-999a-e77bae8c8efe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **user**: What is your task? "
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'________________________________________________________________________________'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> **model**: My task is to act as an attentive and supportive academic assistant. I will answer your questions based solely on the provided excerpts. \n> \n> Here's a breakdown:\n> \n> 1. **Provide Text:** You will give me a question and the relevant text excerpt.\n> 2. **Answer the Question:** I will answer your question using only information found within the provided text.\n> 3. **Detailed Explanation:**  I will explain my answer step-by-step, highlighting the relevant parts of the excerpt that support my response.\n> 4. **\"I Don't Know\":** If the answer cannot be found within the provided excerpts, I will honestly state \"I don't know.\"\n> \n> I am ready to assist you! Please provide me with your question and the related text. \n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'________________________________________________________________________________'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's chat according to the system_instruction\n",
        "\n",
        "* Remember the system_instruction\n",
        "* Our aim is to build a **RAG** pipeline for the future tutorials\n",
        "* Therefore, here, we provide some text and a question relat5ed to the text"
      ],
      "metadata": {
        "id": "rRhmfhVRLvJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "question= \"What is the difference between chat and generate context?\"\n",
        "excerpt= \"\"\" Gemini enables you to have freeform conversations across\n",
        "multiple turns. The ChatSession class simplifies the process\n",
        "by managing the state of the conversation, so unlike with\n",
        "generate_content, you do not have to store the conversation\n",
        "history as a list.\n",
        "\"\"\"\n",
        "prompt = question + excerpt\n",
        "response = chat.send_message(prompt)\n",
        "\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "RLR17eeodoaI",
        "outputId": "f83a13c7-de0c-4ba7-bdad-1a328849dc41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 91.5 ms, sys: 15.7 ms, total: 107 ms\n",
            "Wall time: 5.69 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> The text provided explains the difference between \"chat\" and \"generate_content\" by focusing on conversation management and state. \n> \n> Here's a breakdown:\n> \n> * **Chat:**\n>     * **Freeform conversations:** It allows for natural, back-and-forth interactions across multiple turns.\n>     * **ChatSession class:** This class handles the management of conversation state.\n>     * **No need to store history:**  The ChatSession class automatically maintains the conversation history, eliminating the need for manual storage as required by \"generate_content\".\n> \n> * **Generate Content:**\n>     * **Requires manual history storage:** When using \"generate_content\", the user is responsible for storing the conversation history as a list.\n> \n> **In summary:** The key difference is that \"chat\" leverages the ChatSession class for automated conversation state management, making it simpler and more natural to have multi-turn conversations. On the other hand, \"generate_content\" requires manual history tracking. \n"
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "question= \"What is the difference between chat and generate context?\"\n",
        "excerpt= \"\"\" The generate_content method can handle a wide variety\n",
        "of use cases, including multi-turn chat and multimodal input,\n",
        "depending on what the underlying model supports. The available\n",
        "models only support text and images as input, and text as output.\n",
        "In the simplest case, you can pass a prompt string to the\n",
        "GenerativeModel.generate_content method:\n",
        "\"\"\"\n",
        "prompt = question + excerpt\n",
        "response = chat.send_message(prompt)\n",
        "\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "8hvmm8KEYG_q",
        "outputId": "07e4e220-7d23-44ca-9a20-6ad0a486507d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 140 ms, sys: 12.5 ms, total: 152 ms\n",
            "Wall time: 7.61 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> The text highlights the flexibility of the `generate_content` method while also outlining the limitations of the available models. \n> \n> Here's the breakdown:\n> \n> * **`generate_content` Method:**\n>     * **Versatile:** It can handle various tasks, including multi-turn chat and input from different modalities (text and images).\n>     * **Model-dependent:** The specific capabilities depend on the underlying model used.\n>     * **Current Limitations:** The available models only support text and images as input, and only output text.\n> \n> * **`chat`:** \n>     * This term is not explicitly defined within the provided excerpt.\n> \n> Based on the provided text, the key difference appears to be that the `generate_content` method is a more general tool that can handle different tasks, including multi-turn chat. However, its capabilities are limited by the specific model used. The excerpt doesn't provide information about a separate \"chat\" method, so a direct comparison is not possible. \n"
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "question= \"Summarize the chat so far:\"\n",
        "excerpt= \"\"\n",
        "prompt = question + excerpt\n",
        "response = chat.send_message(prompt)\n",
        "\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "JLkgYrC_a4I3",
        "outputId": "c6cbe67d-7cfc-4743-d533-7269f72aea7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 81.9 ms, sys: 12.8 ms, total: 94.7 ms\n",
            "Wall time: 4.79 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Okay, here's a summary of our chat so far:\n> \n> We started by defining your task as an academic assistant who answers questions based on provided text excerpts.  You then asked for the difference between \"chat\" and \"generate_content\" in the context of Gemini. \n> \n> The first excerpt focused on conversation management, explaining that \"chat\" utilizes the `ChatSession` class for automatic state management, making it easier for multi-turn conversations.  \"Generate_content\" on the other hand, requires users to manually store conversation history.\n> \n> The second excerpt highlighted the versatility of the `generate_content` method, which can handle various tasks including multi-turn chat, but is limited by the capabilities of the underlying model.  The excerpt also mentioned that the current models only support text and images as input and output text.  \n> \n> We are still exploring the differences between \"chat\" and \"generate_content\", as the provided text doesn't explicitly define \"chat\" as a distinct method. \n"
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "question= \"How to stream the chat?\"\n",
        "excerpt= \"\"\n",
        "prompt = question + excerpt\n",
        "response = chat.send_message(prompt)\n",
        "\n",
        "to_markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "PZQac5D_bT8B",
        "outputId": "2695d16b-f5ba-4684-bec9-70924b68c0de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 82.1 ms, sys: 8.73 ms, total: 90.8 ms\n",
            "Wall time: 4.1 s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> I don't know. \n> \n> The provided text excerpts do not contain any information about streaming the chat. \n"
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How Many Tokens --> How much does it cost?\n",
        "\n",
        "https://ai.google.dev/pricing"
      ],
      "metadata": {
        "id": "fcithWncPNyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.count_tokens(chat.history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "TUaFWjL6Eb2e",
        "outputId": "7d8ebecc-d1e8-44b6-f43f-2f248080b2e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "total_tokens: 1108"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build a simple Interface with Gradio"
      ],
      "metadata": {
        "id": "HZDbzRVbC9cE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TaIYCRb-Xndl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "jULTsJpLXlH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_chatBot(system_instruction):\n",
        "  model = genai.GenerativeModel('gemini-1.5-flash-latest', system_instruction=system_instruction)\n",
        "  chat = model.start_chat(history=[])\n",
        "  return chat\n"
      ],
      "metadata": {
        "id": "5ocGWoudYBPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_gemini(prompt, context, chat):\n",
        "  response = chat.send_message(\" Question: \"+ prompt + \" Context: \"+ context)\n",
        "  '''\n",
        "  # Format the chat history for display\n",
        "  formatted_history = \"\\n \".join(\n",
        "        f\"{item.role.capitalize()}: {item.parts if hasattr(item, 'parts') else item.content} \"\n",
        "        for item in chat.history\n",
        "  )\n",
        "  formatted_history = formatted_history.replace(\"[text: \", \"\").replace(\"]\", \"\")\n",
        "  return formatted_history\n",
        "\n",
        "  '''\n",
        "  return response.text\n"
      ],
      "metadata": {
        "id": "1L4M4v_TXmUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_interface(prompt, context):\n",
        "    response = chat_with_gemini(prompt, context, chat)\n",
        "    return response"
      ],
      "metadata": {
        "id": "s9oMtDawx2A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt= \"\"\" You are an attentive and supportive academic assistant.\n",
        "           Your task is to provide assistance based solely on the provided\n",
        "           context. I will provide you the question and related text.\n",
        "           Answer the following questions, ensuring your responses\n",
        "           are derived exclusively from the provided partial texts.\n",
        "           If the answer cannot be found within the provided context,\n",
        "           kindly respond with 'I don't know'.\n",
        "           After answering each question, please provide a detailed\n",
        "           explanation, breaking down the answer step by step and relating\n",
        "           it to the provided context.\n",
        "           If you are ready, I will provide you the question and context.\n",
        "        \"\"\""
      ],
      "metadata": {
        "id": "uoOn6YTzpE5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = build_chatBot(system_prompt)"
      ],
      "metadata": {
        "id": "FzGLkjjuX_AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"What is FC?\"\n",
        "context= \"\"\"FC lets developers create a description\n",
        "of a F in their code, then pass that description to a language\n",
        "model in a request. The response from the model includes the name of\n",
        "a F that matches the description and the arguments to call it with.\n",
        "FC lets you use F as tools in generative AI applications,\n",
        "and you can define more than one F within a single request.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qZ8WNRDCq8JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=chat_with_gemini(prompt, context,chat)\n",
        "to_markdown(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "M_XvbuLwYobL",
        "outputId": "23e38f73-8804-4619-9704-e1d4073906d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> Based on the provided context, **FC** is a tool or a system that allows developers to create descriptions of \"F\" in their code. \n> \n> Here's how I arrived at this answer:\n> \n> 1. The text states, \"FC lets developers create a description of a F in their code...\". \n> 2. This implies that FC is a mechanism or a system that enables the creation of descriptions related to \"F\".\n> 3. The text further elaborates that these descriptions are passed to a language model for processing.\n> 4. Therefore, FC is likely a tool or system designed to facilitate the interaction between code descriptions and language models.\n> \n> **However, the text does not explicitly define what \"F\" refers to.** It's possible that \"F\" is a placeholder for a specific type of function, feature, or object within the context of the code. \n"
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(\n",
        "    fn=chat_interface,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Prompt\", value=prompt),  # Label the prompt input\n",
        "        gr.Textbox(label=\"Context\", value=context)  # Label the excerpt input\n",
        "    ],\n",
        "    outputs=\"markdown\",  # Specify output as markdown\n",
        "    title=\"Chat with Gemini\",\n",
        "    description=\"Type your question with the context to chat with the Gemini model.\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "PLjp8NReZNm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "S0PWcvrJZeJa",
        "outputId": "13504d1f-77aa-4e90-d0f0-0989b0b60b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://b03721ee61247a93b8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b03721ee61247a93b8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://b03721ee61247a93b8.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END OF PART A\n",
        "# A SHORT INTRO GEMINI API FOR TEXT GENERATION & CHAT\n",
        "\n",
        "https://ai.google.dev/gemini-api/docs\n",
        "\n",
        "### In this tutorial, we covered:\n",
        "* The Python SDK for the Gemini API\n",
        "* Check the Google LLM Models available via the provided API\n",
        "* Interact with the models using 2 Alternative Interfaces\n",
        "  1. Generate text interface\n",
        "  2. Interact with the models using Multi-turn conversations (chat) interface\n",
        "\n",
        "* Understand Model & Chat objects\n",
        "  * Model Object in detail\n",
        "  * System Prompt in the Gemini API\n",
        "  * Chat Object in detail\n",
        "* Chat using system_instruction: ***A Manual RAG?***\n",
        "* How Many Tokens --> How much does it cost?\n",
        "* Build a simple Interface with Gradio\n",
        "\n",
        "# In the next tutorial, we will cover ChromaDB as a building block for a RAG pipeline!\n",
        "\n",
        "* Stay tuned!"
      ],
      "metadata": {
        "id": "_bF_bx48Z6VF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Keep learning and commenting!**\n",
        "\n",
        "**Murat Karakaya Akademi YouTube Channel**\n",
        ".\n",
        "\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "."
      ],
      "metadata": {
        "id": "eDAGDu_caQQJ"
      }
    }
  ]
}