{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "Multi Label Model Evaulation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmkarakaya/ML_tutorials/blob/master/Multi_Label_Model_Evaulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqKZHAkgk5OY",
        "colab_type": "text"
      },
      "source": [
        "[SciKit Learn: \n",
        "Model evaluation: quantifying the quality of predictions](https://lijiancheng0614.github.io/scikit-learn/modules/model_evaluation.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW2a1OJ4NPP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ug67UFRlOjv",
        "colab_type": "text"
      },
      "source": [
        "#Synthetic Data:\n",
        "[Calculate mean Average Precision (mAP) for multi-label classification](https://medium.com/@hfdtsinghua/calculate-mean-average-precision-map-for-multi-label-classification-b082679d31be)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ByXG3zppQ_B",
        "colab_type": "text"
      },
      "source": [
        "First assume that in the data set we have only 3 samples as below. Each sample has 4 classes: A, B, C, and D. \n",
        "In samples more than one classes exist! Therefore, the problem multi-label classification!\n",
        "\n",
        "After some training, some ML model produce the predicted scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfQJjLDmnw8m",
        "colab_type": "text"
      },
      "source": [
        "![Sample Data with scores](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/sample%20mutli%20label%20classification%20scores.png?raw=true)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi_qgCxFlNeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "y_true = np.array([[0, 1, 1, 1],[0,0,1,0],[1,1,0,0]])\n",
        "y_scores = np.array([[0.2, 0.6, 0.1, 0.8],[0.4,0.9,0.8,0.6],[0.8,0.4,0.5,0.7]])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4oWUGdVu6rx",
        "colab_type": "text"
      },
      "source": [
        "#Threshold: Let's assume we are using **0.5 as the threshold** for prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS4XSS8YvHcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "threshold = 0.5\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8WBPN1OxXmw",
        "colab_type": "code",
        "outputId": "f811bcf2-661e-4c23-c80a-16608d438970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "y_pred=[]\n",
        "for sample in  y_scores:\n",
        "  y_pred.append([1 if i>=0.5 else 0 for i in sample ] )\n",
        "y_pred = np.array(y_pred)\n",
        "y_pred"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 1],\n",
              "       [0, 1, 1, 1],\n",
              "       [1, 0, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JANvg0Post6n",
        "colab_type": "text"
      },
      "source": [
        "# [Accuracy score](http://lijiancheng0614.github.io/scikit-learn/modules/model_evaluation.html#accuracy-score)\n",
        "\n",
        "The accuracy_score function computes the accuracy, either the fraction (default) or the count (normalize=False) of correct predictions.\n",
        "\n",
        "**In multilabel classification**, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is **0.0**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuegY_yhsfX7",
        "colab_type": "code",
        "outputId": "d3959bcc-3bc7-4576-d70d-c382a7254733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_true, y_pred)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZoikTSK1a-l",
        "colab_type": "text"
      },
      "source": [
        "**Notice**: Here, accuracy is based on if all the 4-class prediction is correct or naot.\n",
        "\n",
        "It is not label-based!\n",
        "\n",
        "Therefore accuracy is zero!\n",
        "\n",
        "We have to use better metrics for multilabel classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV7ce4F_5PLU",
        "colab_type": "text"
      },
      "source": [
        "# [Confusion matrix](http://lijiancheng0614.github.io/scikit-learn/modules/model_evaluation.html#confusion-matrix) \n",
        "\n",
        "The confusion_matrix function evaluates classification accuracy by computing the confusion matrix.\n",
        "\n",
        "By definition, entry i, j in a confusion matrix is the number of observations actually in group i, but predicted to be in group j. \n",
        "\n",
        "![Sample Confusion Matrix](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/confusion%20matrix%20sample.png?raw=true)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyRqnM_M5OWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "#confusion_matrix(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "torLpxmT7uvg",
        "colab_type": "text"
      },
      "source": [
        "#[Multilabel confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html?highlight=multilabel#sklearn-metrics-multilabel-confusion-matrix)\n",
        "\n",
        "\n",
        "* Compute a confusion matrix for each class or sample \n",
        "* **NOTICE:** The cells has different meaning from Binary Class Confusion Matrix! \n",
        "\n",
        "![MCM](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/MCM%20confusion%20matrix%20sample.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JxZx_hj_K0Y",
        "colab_type": "code",
        "outputId": "9eed46f3-a1c2-47ed-bd2e-7938a5370502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(\"Actual \\n\", y_true)\n",
        "print(\"\\nPredicted \\n\",y_pred)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual \n",
            " [[0 1 1 1]\n",
            " [0 0 1 0]\n",
            " [1 1 0 0]]\n",
            "\n",
            "Predicted \n",
            " [[0 1 0 1]\n",
            " [0 1 1 1]\n",
            " [1 0 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpaqgMh71aOv",
        "colab_type": "code",
        "outputId": "45ecf554-1bbe-4e01-9b8c-5446dc63e6e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "multilabel_confusion_matrix(y_true, y_pred)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[2, 0],\n",
              "        [0, 1]],\n",
              "\n",
              "       [[0, 1],\n",
              "        [1, 1]],\n",
              "\n",
              "       [[0, 1],\n",
              "        [1, 1]],\n",
              "\n",
              "       [[0, 2],\n",
              "        [0, 1]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I17SFfwGD8Sa",
        "colab_type": "text"
      },
      "source": [
        "#[Classification report](http://lijiancheng0614.github.io/scikit-learn/modules/model_evaluation.html#classification-report)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgaojYsXD7m-",
        "colab_type": "code",
        "outputId": "7a1eed98-68e4-4d91-de81-25c2f2daa771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "label_names = ['label A', 'label B', 'label C', 'label D']\n",
        "\n",
        "print(classification_report(y_true, y_pred,target_names=label_names))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     label A       1.00      1.00      1.00         1\n",
            "     label B       0.50      0.50      0.50         2\n",
            "     label C       0.50      0.50      0.50         2\n",
            "     label D       0.33      1.00      0.50         1\n",
            "\n",
            "   micro avg       0.50      0.67      0.57         6\n",
            "   macro avg       0.58      0.75      0.62         6\n",
            "weighted avg       0.56      0.67      0.58         6\n",
            " samples avg       0.56      0.72      0.57         6\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MUfUsfyQLbQ",
        "colab_type": "text"
      },
      "source": [
        "#[Precision, recall and F-measures](http://lijiancheng0614.github.io/scikit-learn/modules/model_evaluation.html#precision-recall-and-f-measures)\n",
        "\n",
        "\n",
        "Intuitively, precision is the ability of the classifier not to label as positive a sample that is negative, and recall is the ability of the classifier to find all the positive samples.\n",
        "\n",
        "The F-measure can be interpreted as a weighted harmonic mean of the precision and recall:  reaches its best value at 1 and its worst score at 0.\n",
        "\n",
        "Several functions allow you to analyze the precision, recall and F-measures score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4iBlAQIQven",
        "colab_type": "text"
      },
      "source": [
        "#[Precision, recall and F-measures @ Multilabel classification ](http://lijiancheng0614.github.io/scikit-learn/modules/model_evaluation.html#multiclass-and-multilabel-classification)\n",
        "\n",
        "\n",
        "* In multiclass and multilabel classification task, the notions of precision, \n",
        "recall, and F-measures can be applied to each label independently. \n",
        "\n",
        "* There are a few ways to combine results across labels, specified by the ***average*** argument to the average_precision_score (multilabel only), f1_score, fbeta_score, precision_recall_fscore_support, precision_score and recall_score functions, as described above. \n",
        "\n",
        "* Note that for ***“micro”***-averaging in a multiclass setting with all labels included will produce equal precision, recall and F, while ***“weighted”*** averaging may produce an F-score that is not between precision and recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9Em4uWRTCf-",
        "colab_type": "text"
      },
      "source": [
        "##Precision\n",
        "[precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn-metrics-precision-score)\n",
        "\n",
        "\n",
        "* The precision is the ratio **tp / (tp + fp)** where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
        "\n",
        "* The best value is 1 and the worst value is 0.\n",
        "\n",
        "\n",
        "***average*** parameter is required for multiclass/multilabel targets. \n",
        "\n",
        "* **None**, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n",
        "\n",
        "* '**micro**':\n",
        "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
        "\n",
        "* '**macro**':\n",
        "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
        "\n",
        "* '**weighted**':\n",
        "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
        "\n",
        "* '**samples**':\n",
        "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LUSgldhTBvQ",
        "colab_type": "code",
        "outputId": "608f9547-2f72-40aa-d78d-df4d5b8ff179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(\"Actual \\n\", y_true)\n",
        "print(\"\\nPredicted \\n\",y_pred)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual \n",
            " [[0 1 1 1]\n",
            " [0 0 1 0]\n",
            " [1 1 0 0]]\n",
            "\n",
            "Predicted \n",
            " [[0 1 0 1]\n",
            " [0 1 1 1]\n",
            " [1 0 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK2iOH_6xHzS",
        "colab_type": "text"
      },
      "source": [
        "####Precision of each label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SSVg4wub9qP",
        "colab_type": "code",
        "outputId": "8e21fd62-28b1-4da8-f3f3-36e851d8bbab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "print(\"None \", metrics.precision_score(y_true, y_pred, average=None))  \n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None  [1.         0.5        0.5        0.33333333]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Yb2_DVhxrc5",
        "colab_type": "text"
      },
      "source": [
        "###Average Precision\n",
        "[precision_score funtion has](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn-metrics-precision-score)\n",
        "\n",
        "***average*** parameter is required for multiclass/multilabel targets. \n",
        "\n",
        "* **None**, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n",
        "\n",
        "* '**micro**':\n",
        "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
        "\n",
        "* '**macro**':\n",
        "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
        "\n",
        "* '**weighted**':\n",
        "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
        "\n",
        "* '**samples**':\n",
        "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brRnFggmuqGB",
        "colab_type": "text"
      },
      "source": [
        "#### Use precision_score function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6voTYj5YQKuS",
        "colab_type": "code",
        "outputId": "202b4f6c-47d7-42a2-ed0d-1504a8758598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "\n",
        "print(\"micro: {:.2f}\".format(metrics.precision_score(y_true, y_pred, average='micro')))\n",
        "print(\"macro: {:.2f} \".format( metrics.precision_score(y_true, y_pred, average='macro')))\n",
        "print(\"weighted: {:.2f} \".format( metrics.precision_score(y_true, y_pred, average='weighted')))\n",
        "print(\"samples: {:.2f} \".format( metrics.precision_score(y_true, y_pred, average='samples')))  \n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "micro: 0.50\n",
            "macro: 0.58 \n",
            "weighted: 0.56 \n",
            "samples: 0.56 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTsM9OVR3f1x",
        "colab_type": "text"
      },
      "source": [
        "####  [Use average_precision_score function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn-metrics-average-precision-score)\n",
        "\n",
        "* average_precision_score(y_true, **y_score** , average='macro', sample_weight=None)\n",
        "\n",
        "*Compute average precision (AP) from prediction scores\n",
        "\n",
        "* average_precision_score summarizes a **precision-recall curve** as the weighted mean of precisions achieved **at each threshold**, with the increase in recall from the previous threshold used as the weight:\n",
        "\n",
        ">>$\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n$\n",
        " \n",
        "where  and  are the precision and recall at the nth threshold [1]. \n",
        "* This implementation is not interpolated and is different from computing the area under the precision-recall curve with the trapezoidal rule, which uses linear interpolation and can be too optimistic.\n",
        "\n",
        "* Note: this implementation is restricted to the binary classification task or **multilabel** classification task.\n",
        "\n",
        "**IMPORTANT:** \n",
        "* Precision refers to precision at a **particular decision threshold**. For example, if you count any model output less than 0.5 as negative, and greater than 0.5 as positive. \n",
        "* But sometimes (especially if your classes are not balanced, or if you want to favor precision over recall or vice versa), you may want to **vary this threshold**. **average_precision_score function** gives you ***average precision at all such possible thresholds***, which is also similar to the ***area under the precision-recall curve***. \n",
        "* It is a useful metric to compare how well models are ordering the predictions, without considering any specific decision threshold.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHEge9pB6Agn",
        "colab_type": "code",
        "outputId": "d90bf7fd-82cc-48e6-bb10-2a11921cc20f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "y_scores"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.2, 0.6, 0.1, 0.8],\n",
              "       [0.4, 0.9, 0.8, 0.6],\n",
              "       [0.8, 0.4, 0.5, 0.7]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynqnE2p3Vf3Y",
        "colab_type": "code",
        "outputId": "3f5db7e5-0b95-42c9-e8aa-c2e9758787cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"micro: {:.2f}\".format(metrics.average_precision_score(y_true, y_scores, average='micro')))\n",
        "print(\"macro: {:.2f} \".format( metrics.average_precision_score(y_true, y_scores, average='macro')))\n",
        "print(\"weighted: {:.2f} \".format( metrics.average_precision_score(y_true, y_scores, average='weighted')))\n",
        "print(\"samples: {:.2f} \".format( metrics.average_precision_score(y_true, y_scores, average='samples')))  "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "micro: 0.64\n",
            "macro: 0.85 \n",
            "weighted: 0.81 \n",
            "samples: 0.72 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4us93CLlVjSG",
        "colab_type": "text"
      },
      "source": [
        "#### Calculate micro Precision \n",
        "* Calculate metrics **globally** by counting the total true positives and false positives.\n",
        "\n",
        "The precision is the ratio tp / (tp + fp) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyZpgSccmqG1",
        "colab_type": "text"
      },
      "source": [
        "##### Count by yourself"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei3z0Y1Yi0b1",
        "colab_type": "code",
        "outputId": "e4bc2e24-de44-4c3e-e990-91e2acbfdefb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "np.concatenate((y_true.reshape(12,1), y_pred.reshape(12,1)), axis=1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0],\n",
              "       [1, 1],\n",
              "       [1, 0],\n",
              "       [1, 1],\n",
              "       [0, 0],\n",
              "       [0, 1],\n",
              "       [1, 1],\n",
              "       [0, 1],\n",
              "       [1, 1],\n",
              "       [1, 0],\n",
              "       [0, 1],\n",
              "       [0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0xJUqMQVg5E",
        "colab_type": "code",
        "outputId": "bbc71b90-556e-4cdf-8d0e-ff508c52702f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "TP=((y_true * y_pred) == 1).sum()\n",
        "print(\"TP: \", TP)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TP:  4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCCBTjiTkHGm",
        "colab_type": "code",
        "outputId": "b186b306-d8d1-4b1e-9156-f2d3d789363b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "converted_y_true= np.copy(y_true)\n",
        "converted_y_true[converted_y_true==1] = 5\n",
        "converted_y_true[converted_y_true==0] = 1\n",
        "converted_y_true[converted_y_true==5] = 0\n",
        "FP= ((converted_y_true * y_pred)== 1).sum()\n",
        "print(\"FP: \", FP)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FP:  4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUTNZw_jnaGF",
        "colab_type": "code",
        "outputId": "8da7cd6b-38b4-4bec-970d-34944320d855",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\" Micro Precision {:.2f}\".format(TP/(TP+FP)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Micro Precision 0.50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMD5-FYvmwz3",
        "colab_type": "text"
      },
      "source": [
        "##### Flatten & use as in binary classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjXXvSElmlks",
        "colab_type": "code",
        "outputId": "27d07a00-75f1-4548-a6a8-4a9d23ed5d51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Micro Precision: {:.2f}\".format(metrics.precision_score(y_true.ravel(), y_pred.ravel())))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Micro Precision: 0.50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHCVGTQBbTAN",
        "colab_type": "text"
      },
      "source": [
        "#### Calculate macro Precision \n",
        "* Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
        "\n",
        "The precision is the ratio tp / (tp + fp) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3xfOUsblgWo",
        "colab_type": "code",
        "outputId": "21f3dc1a-6c8c-47a2-adf7-687dcfb10d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "totalPrecision= 0\n",
        "for i in range (len(label_names)):\n",
        "  p= metrics.precision_score(y_true[:,i], y_pred[:,i])\n",
        "  totalPrecision+= p\n",
        "  print(\"For {} precision: {:.2f}\".format(label_names[i], p))\n",
        "print(\"Macro Precision: {:.2f}\".format(totalPrecision/len(label_names)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For label A precision: 1.00\n",
            "For label B precision: 0.50\n",
            "For label C precision: 0.50\n",
            "For label D precision: 0.33\n",
            "Macro Precision: 0.58\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J9ksrGUnqFG4"
      },
      "source": [
        "#### Calculate weighted Precision \n",
        "*  Calculate metrics for each label, and find their average **weighted by support** (the number of true instances for each label). \n",
        "*  This alters ‘macro’ to account for label **imbalance**; it can result in an *F-score that is not between precision and recall*.\n",
        "\n",
        "The precision is the ratio tp / (tp + fp) \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8f52g6-qix9",
        "colab_type": "code",
        "outputId": "dc82f714-1d57-4a92-daa0-6d49aed64d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "totalPrecision=0\n",
        "totalSupport=0\n",
        "for i in range (len(label_names)):\n",
        "  p= metrics.precision_score(y_true[:,i], y_pred[:,i])\n",
        "  support= (y_true[:,i]==1).sum()\n",
        "  totalSupport+=support\n",
        "  totalPrecision+= p*support\n",
        "  print(\"For {} precision: {:.2f} support: {}\".format(label_names[i], p, support ))\n",
        "print(\"Weighted Precision: {:.2f}\".format(totalPrecision/totalSupport))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For label A precision: 1.00 support: 1\n",
            "For label B precision: 0.50 support: 2\n",
            "For label C precision: 0.50 support: 2\n",
            "For label D precision: 0.33 support: 1\n",
            "Weighted Precision: 0.56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ6Sqfj5tkYC",
        "colab_type": "text"
      },
      "source": [
        "#### Calculate samples Precision \n",
        "\n",
        "* Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf9zjgAuyy8Z",
        "colab_type": "text"
      },
      "source": [
        "##Recall\n",
        "[recall_score](http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.metrics.recall_score.html#sklearn-metrics-recall-score)\n",
        "\n",
        "\n",
        "* Compute the recall: The recall is the ratio **tp / (tp + fn)** where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
        "\n",
        "* The best value is 1 and the worst value is 0.\n",
        "\n",
        "\n",
        "***average*** parameter is required for multiclass/multilabel targets. \n",
        "\n",
        "* **None**, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n",
        "\n",
        "* '**micro**':\n",
        "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
        "\n",
        "* '**macro**':\n",
        "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
        "\n",
        "* '**weighted**':\n",
        "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
        "\n",
        "* '**samples**':\n",
        "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC4olLOAzzcW",
        "colab_type": "text"
      },
      "source": [
        "### Recall of each label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abwQfDgcz7ua",
        "colab_type": "code",
        "outputId": "d3d68570-9784-4fa6-b23c-fdb1642a1700",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Recall of each label: {}\".format(metrics.recall_score(y_true, y_pred, average=None)))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall of each label: [1.  0.5 0.5 1. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhwKB72T0ObO",
        "colab_type": "text"
      },
      "source": [
        "### Average Recall"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRYt0RY-TAHr",
        "colab_type": "code",
        "outputId": "97aaeb51-8a94-463f-c83c-d8cb1132aedd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"micro: {:.2f}\".format(metrics.recall_score(y_true, y_pred, average='micro')))\n",
        "print(\"macro: {:.2f} \".format( metrics.recall_score(y_true, y_pred, average='macro')))\n",
        "print(\"weighted: {:.2f} \".format( metrics.recall_score(y_true, y_pred, average='weighted')))\n",
        "print(\"samples: {:.2f} \".format( metrics.recall_score(y_true, y_pred, average='samples')))  "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "micro: 0.67\n",
            "macro: 0.75 \n",
            "weighted: 0.67 \n",
            "samples: 0.72 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUkjouJZ1DVo",
        "colab_type": "text"
      },
      "source": [
        "##F1 score\n",
        "[f1_score](http://lijiancheng0614.github.io/scikit-learn/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)\n",
        "\n",
        "\n",
        "* Compute the F1 score, also known as balanced F-score or F-measure\n",
        "\n",
        "* The F1 score can be interpreted as a ***weighted average of the precision and recall***, where an F1 score reaches its best value at 1 and worst score at 0. \n",
        "\n",
        "* The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
        "\n",
        "> >>`F1 = 2 * (precision * recall) / (precision + recall)`\n",
        "\n",
        "\n",
        "In the multi-class and **multi-label** case, this is the** weighted average of the F1 score of each class**.\n",
        "\n",
        "\n",
        "***average*** parameter is required for multiclass/multilabel targets. \n",
        "\n",
        "* **None**, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n",
        "\n",
        "* '**micro**':\n",
        "Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
        "\n",
        "* '**macro**':\n",
        "Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
        "\n",
        "* '**weighted**':\n",
        "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nvFtRZ52LqM",
        "colab_type": "text"
      },
      "source": [
        "### F1 of each label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZyJhPtk2I8C",
        "colab_type": "code",
        "outputId": "cc01cfdd-5e7d-4a9d-ebab-b48ea8d1a7e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"F1 of each label: {}\".format(metrics.f1_score(y_true, y_pred, average=None)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 of each label: [1.  0.5 0.5 0.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB9zCQZ_2ZtA",
        "colab_type": "text"
      },
      "source": [
        "### Average F1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngMnEJn22Knv",
        "colab_type": "code",
        "outputId": "44b0731b-f4dc-4bd2-a08a-103f1373b123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"micro: {:.2f}\".format(metrics.f1_score(y_true, y_pred, average='micro')))\n",
        "print(\"macro: {:.2f} \".format( metrics.f1_score(y_true, y_pred, average='macro')))\n",
        "print(\"weighted: {:.2f} \".format( metrics.f1_score(y_true, y_pred, average='weighted')))\n",
        "print(\"samples: {:.2f} \".format( metrics.f1_score(y_true, y_pred, average='samples')))  "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "micro: 0.57\n",
            "macro: 0.62 \n",
            "weighted: 0.58 \n",
            "samples: 0.57 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyiQ7VWrA0jR",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* **Remember**: We assumed that we are using **0.5 as the threshold** for prediction during all the computations above.\n",
        "\n",
        "* What hapens if we select **different** values as the threshold?\n",
        "\n",
        "Let's see!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwdlkbtHBwwS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "83b63621-6f33-4e5a-d33c-7558bafebb77"
      },
      "source": [
        "print(\"y_true: \\n{}\".format(y_true))\n",
        "print(\"y_scores: \\n{}\".format(y_scores))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_true: \n",
            "[[0 1 1 1]\n",
            " [0 0 1 0]\n",
            " [1 1 0 0]]\n",
            "y_scores: \n",
            "[[0.2 0.6 0.1 0.8]\n",
            " [0.4 0.9 0.8 0.6]\n",
            " [0.8 0.4 0.5 0.7]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUr9pSLGCG2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predictLabelForGivenThreshold(threshold):\n",
        "  y_pred=[]\n",
        "  for sample in  y_scores:\n",
        "    y_pred.append([1 if i>=threshold else 0 for i in sample ] )\n",
        "  return np.array(y_pred)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycJwx8OrDAl6",
        "colab_type": "text"
      },
      "source": [
        "### Classification Report when threshold = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6-f50NTCfjq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "b1ff34ff-2c04-44a6-ef0d-db4ef81864d8"
      },
      "source": [
        "y_pred  = predictLabelForGivenThreshold(0.5)\n",
        "print(classification_report(y_true, y_pred,target_names=label_names))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     label A       1.00      1.00      1.00         1\n",
            "     label B       0.50      0.50      0.50         2\n",
            "     label C       0.50      0.50      0.50         2\n",
            "     label D       0.33      1.00      0.50         1\n",
            "\n",
            "   micro avg       0.50      0.67      0.57         6\n",
            "   macro avg       0.58      0.75      0.62         6\n",
            "weighted avg       0.56      0.67      0.58         6\n",
            " samples avg       0.56      0.72      0.57         6\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7QQURxEDKgn",
        "colab_type": "text"
      },
      "source": [
        "### Classification Report when threshold = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFHVqB6fDJyA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "6beceb39-1c21-4780-d463-27bd25cf3e48"
      },
      "source": [
        "y_pred  = predictLabelForGivenThreshold(0.2)\n",
        "print(classification_report(y_true, y_pred,target_names=label_names))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     label A       0.33      1.00      0.50         1\n",
            "     label B       0.67      1.00      0.80         2\n",
            "     label C       0.50      0.50      0.50         2\n",
            "     label D       0.33      1.00      0.50         1\n",
            "\n",
            "   micro avg       0.45      0.83      0.59         6\n",
            "   macro avg       0.46      0.88      0.57         6\n",
            "weighted avg       0.50      0.83      0.60         6\n",
            " samples avg       0.47      0.89      0.58         6\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F6hT-RzDNmf",
        "colab_type": "text"
      },
      "source": [
        "### Classification Report when threshold = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iLWmsHeDTRX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "cd6380c6-459c-484f-e0c9-facc0719f308"
      },
      "source": [
        "y_pred  = predictLabelForGivenThreshold(0.8)\n",
        "print(classification_report(y_true, y_pred,target_names=label_names))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     label A       1.00      1.00      1.00         1\n",
            "     label B       0.00      0.00      0.00         2\n",
            "     label C       1.00      0.50      0.67         2\n",
            "     label D       1.00      1.00      1.00         1\n",
            "\n",
            "   micro avg       0.75      0.50      0.60         6\n",
            "   macro avg       0.75      0.62      0.67         6\n",
            "weighted avg       0.67      0.50      0.56         6\n",
            " samples avg       0.83      0.61      0.61         6\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLTSn1q6D184",
        "colab_type": "text"
      },
      "source": [
        "### Which threshold?\n",
        "\n",
        "* Which threshold yileds better result?\n",
        "* Which metric do you care?\n",
        "* How can you compare different models' success/performance?\n",
        "* Which precision/recall/f1 value to report?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3gi948Isd-w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "bde978a4-4588-42de-b9c8-6e59e5ddd9d3"
      },
      "source": [
        "from sklearn.metrics import \n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(\n",
        "    y_true, y_scores)\n",
        "print(precision)\n",
        "print(recall)\n",
        "print(thresholds)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-c16dc662780b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from sklearn.metrics import\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9U5_X4NNPP8",
        "colab_type": "text"
      },
      "source": [
        "# Precision-Recall Curve\n",
        "\n",
        "\n",
        "Example of Precision-Recall metric to evaluate classifier output quality.\n",
        "\n",
        "Precision-Recall is a useful measure of success of prediction when the\n",
        "classes are very imbalanced. In information retrieval, precision is a\n",
        "measure of result relevancy, while recall is a measure of how many truly\n",
        "relevant results are returned.\n",
        "\n",
        "The precision-recall curve shows the tradeoff between precision and\n",
        "recall for different threshold. A high area under the curve represents\n",
        "both high recall and high precision, where high precision relates to a\n",
        "low false positive rate, and high recall relates to a low false negative\n",
        "rate. High scores for both show that the classifier is returning accurate\n",
        "results (high precision), as well as returning a majority of all positive\n",
        "results (high recall).\n",
        "\n",
        "A system with high recall but low precision returns many results, but most of\n",
        "its predicted labels are incorrect when compared to the training labels. A\n",
        "system with high precision but low recall is just the opposite, returning very\n",
        "few results, but most of its predicted labels are correct when compared to the\n",
        "training labels. An ideal system with high precision and high recall will\n",
        "return many results, with all results labeled correctly.\n",
        "\n",
        "Precision ($P$) is defined as the number of true positives ($T_p$)\n",
        "over the number of true positives plus the number of false positives\n",
        "($F_p$).\n",
        "\n",
        "$P = \\frac{T_p}{T_p+F_p}$\n",
        "\n",
        "Recall ($R$) is defined as the number of true positives ($T_p$)\n",
        "over the number of true positives plus the number of false negatives\n",
        "($F_n$).\n",
        "\n",
        "$R = \\frac{T_p}{T_p + F_n}$\n",
        "\n",
        "These quantities are also related to the ($F_1$) score, which is defined\n",
        "as the harmonic mean of precision and recall.\n",
        "\n",
        "$F1 = 2\\frac{P \\times R}{P+R}$\n",
        "\n",
        "Note that the precision may not decrease with recall. The\n",
        "definition of precision ($\\frac{T_p}{T_p + F_p}$) shows that lowering\n",
        "the threshold of a classifier may increase the denominator, by increasing the\n",
        "number of results returned. If the threshold was previously set too high, the\n",
        "new results may all be true positives, which will increase precision. If the\n",
        "previous threshold was about right or too low, further lowering the threshold\n",
        "will introduce false positives, decreasing precision.\n",
        "\n",
        "Recall is defined as $\\frac{T_p}{T_p+F_n}$, where $T_p+F_n$ does\n",
        "not depend on the classifier threshold. This means that lowering the classifier\n",
        "threshold may increase recall, by increasing the number of true positive\n",
        "results. It is also possible that lowering the threshold may leave recall\n",
        "unchanged, while the precision fluctuates.\n",
        "\n",
        "The relationship between recall and precision can be observed in the\n",
        "stairstep area of the plot - at the edges of these steps a small change\n",
        "in the threshold considerably reduces precision, with only a minor gain in\n",
        "recall.\n",
        "\n",
        "**Average precision** (AP) summarizes such a plot as the weighted mean of\n",
        "precisions achieved at each threshold, with the increase in recall from the\n",
        "previous threshold used as the weight:\n",
        "\n",
        "$\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n$\n",
        "\n",
        "where $P_n$ and $R_n$ are the precision and recall at the\n",
        "nth threshold. A pair $(R_k, P_k)$ is referred to as an\n",
        "*operating point*.\n",
        "\n",
        "AP and the trapezoidal area under the operating points\n",
        "(:func:`sklearn.metrics.auc`) are common ways to summarize a precision-recall\n",
        "curve that lead to different results. Read more in the\n",
        "`User Guide <precision_recall_f_measure_metrics>`.\n",
        "\n",
        "Precision-recall curves are typically used in binary classification to study\n",
        "the output of a classifier. In order to extend the precision-recall curve and\n",
        "average precision to multi-class or multi-label classification, it is necessary\n",
        "to binarize the output. One curve can be drawn per label, but one can also draw\n",
        "a precision-recall curve by considering each element of the label indicator\n",
        "matrix as a binary prediction (micro-averaging).\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>See also :func:`sklearn.metrics.average_precision_score`,\n",
        "             :func:`sklearn.metrics.recall_score`,\n",
        "             :func:`sklearn.metrics.precision_score`,\n",
        "             :func:`sklearn.metrics.f1_score`</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8tgsUdJNPP8",
        "colab_type": "text"
      },
      "source": [
        "In binary classification settings\n",
        "--------------------------------------------------------\n",
        "\n",
        "Create simple data\n",
        "..................\n",
        "\n",
        "Try to differentiate the two first classes of the iris data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkoZ5aY9NPP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlpMitE0OMuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.shape\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE0n9UwJOFas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add noisy features\n",
        "random_state = np.random.RandomState(0)\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "random_features= random_state.randn(n_samples, 200 * n_features)\n",
        "X = np.c_[X,random_features ]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxsYP0BlPLGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X.shape\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoGNcwRKPLP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Limit to the two first classes, and split into training and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X[y < 2], y[y < 2],\n",
        "                                                    test_size=.5,\n",
        "                                                    random_state=random_state)\n",
        "\n",
        "# Create a simple classifier\n",
        "classifier = svm.LinearSVC(random_state=random_state)\n",
        "classifier.fit(X_train, y_train)\n",
        "y_score = classifier.decision_function(X_test)\n",
        "y_predict = classifier.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cDvVXsQRrC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(y_score[:5])\n",
        "print(y_predict[:5])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8LcyCN-NPQA",
        "colab_type": "text"
      },
      "source": [
        "Compute the average precision score\n",
        "...................................\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgNxMWSeNPQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import average_precision_score\n",
        "average_precision = average_precision_score(y_test, y_score)\n",
        "\n",
        "print('Average precision-recall score: {0:0.2f}'.format(\n",
        "      average_precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULSdr9zyWDQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orOUpeLpNPQD",
        "colab_type": "text"
      },
      "source": [
        "Plot the Precision-Recall curve\n",
        "................................\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QafOhGtcNPQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import plot_precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "disp = plot_precision_recall_curve(classifier, X_test, y_test)\n",
        "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
        "                   'AP={0:0.2f}'.format(average_precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEkdc26rNPQH",
        "colab_type": "text"
      },
      "source": [
        "In multi-label settings\n",
        "------------------------\n",
        "\n",
        "Create multi-label data, fit, and predict\n",
        "...........................................\n",
        "\n",
        "We create a multi-label dataset, to illustrate the precision-recall in\n",
        "multi-label settings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBmgrtOqNPQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Use label_binarize to be multi-label like settings\n",
        "Y = label_binarize(y, classes=[0, 1, 2])\n",
        "n_classes = Y.shape[1]\n",
        "\n",
        "# Split into training and test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5,\n",
        "                                                    random_state=random_state)\n",
        "\n",
        "# We use OneVsRestClassifier for multi-label prediction\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Run classifier\n",
        "classifier = OneVsRestClassifier(svm.LinearSVC(random_state=random_state))\n",
        "classifier.fit(X_train, Y_train)\n",
        "y_score = classifier.decision_function(X_test)\n",
        "y_predict =  classifier.predict(X_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSQm_FT-N0wW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(y_score[:5])\n",
        "print(y_predict[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qutFjTSlNPQM",
        "colab_type": "text"
      },
      "source": [
        "The average precision score in multi-label settings\n",
        "....................................................\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTwn_6oiNPQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "# For each class\n",
        "precision = dict()\n",
        "recall = dict()\n",
        "average_precision = dict()\n",
        "for i in range(n_classes):\n",
        "    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],\n",
        "                                                        y_score[:, i])\n",
        "    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])\n",
        "\n",
        "# A \"micro-average\": quantifying score on all classes jointly\n",
        "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n",
        "    y_score.ravel())\n",
        "average_precision[\"micro\"] = average_precision_score(Y_test, y_score,\n",
        "                                                     average=\"micro\")\n",
        "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
        "      .format(average_precision[\"micro\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWArVpUBfE0B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79AOqlKgfFKE",
        "colab_type": "text"
      },
      "source": [
        "### Synthetic Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UPLTag3zGvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true = np.array([0, 0, 1, 1])\n",
        "y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
        "precision, recall, thresholds = precision_recall_curve(\n",
        "    y_true, y_scores)\n",
        "print(precision)\n",
        "print(recall)\n",
        "print(thresholds)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9beRh8mw3qK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "#plt.step(np.append(0.0,thresholds), np.flip(precision), where='post')\n",
        "plt.step(np.append(0.0,thresholds), np.flip(recall), where='post')\n",
        "plt.xlabel('Treshold')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title(' precision  vs threshold ')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t58nvO7nIJPE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For each class\n",
        "precision = dict()\n",
        "recall = dict()\n",
        "treshold = dict()\n",
        "average_precision = dict()\n",
        "\n",
        "\n",
        "Y_actual= np.array([[0,1,1,1], [0,0,1,0], [1,1,0,0]], dtype=np.int)\n",
        "y_prediction= np.array([[0.2,0.6,0.1,0.8],[0.4,0.9,0.8,0.6],[0.8,0.4,0.5,0.7]], dtype=np.float64)\n",
        "\n",
        "n_classes = Y_actual.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qADJPjQOlTc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(n_classes):\n",
        "    precision[i], recall[i], treshold[i] = precision_recall_curve(Y_actual[:, i], y_prediction[:, i])\n",
        "    average_precision[i] =average_precision_score(Y_actual[:, i], y_prediction[:, i])\n",
        "\n",
        "print (\"average_precision {} \".format(average_precision))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dyoTmqZqD0a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=np.append(treshold[2],1)\n",
        "\n",
        "print(x)\n",
        "print((precision[2]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYHXbYbwnHfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "for i in range(n_classes):\n",
        "  plt.step(np.append(treshold[i],1), precision[i], where='post')\n",
        "  plt.xlabel('Treshold')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.title(\n",
        "    'Average precision score for class {}: AP={:.3f}'.format(i, average_precision[i]))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFxSX_tRlYKv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "for i in range(n_classes):\n",
        "  plt.step(recall[i], precision[i], where='post')\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.title(\n",
        "    'Average precision score for class {}: AP={:.3f}'.format(i, average_precision[i]))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaKrbEcdaqDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# A \"micro-average\": quantifying score on all classes jointly\n",
        "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_actual.ravel(),\n",
        "    y_prediction.ravel())\n",
        "average_precision[\"None\"] = average_precision_score(Y_actual, y_prediction, average=None)\n",
        "average_precision[\"Void\"] = average_precision_score(Y_actual, y_prediction)\n",
        "average_precision[\"micro\"] = average_precision_score(Y_actual, y_prediction, average=\"micro\")\n",
        "average_precision[\"macro\"] = average_precision_score(Y_actual, y_prediction, average=\"macro\")\n",
        "average_precision[\"samples\"] = average_precision_score(Y_actual, y_prediction, average=\"samples\")\n",
        "average_precision[\"weighted\"] = average_precision_score(Y_actual, y_prediction, average=\"weighted\")\n",
        "\n",
        "#print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision[\"micro\"]))\n",
        "    \n",
        "average_precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63DZQQA8P770",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "precision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXVi6JmuNPQQ",
        "colab_type": "text"
      },
      "source": [
        "Plot the micro-averaged Precision-Recall curve\n",
        "...............................................\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_hWUHgvNPQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.step(recall['micro'], precision['micro'], where='post')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title(\n",
        "    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
        "    .format(average_precision[\"micro\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgYN9E2JNPQT",
        "colab_type": "text"
      },
      "source": [
        "Plot Precision-Recall curve for each class and iso-f1 curves\n",
        ".............................................................\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-ymEilJ_I3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.linspace(0.01, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmyw68IwNPQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import cycle\n",
        "# setup plot details\n",
        "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
        "\n",
        "plt.figure(figsize=(7, 8))\n",
        "f_scores = np.linspace(0.2, 0.8, num=4)\n",
        "lines = []\n",
        "labels = []\n",
        "for f_score in f_scores:\n",
        "    x = np.linspace(0.01, 1)\n",
        "    y = f_score * x / (2 * x - f_score)\n",
        "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
        "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
        "\n",
        "lines.append(l)\n",
        "labels.append('iso-f1 curves')\n",
        "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
        "lines.append(l)\n",
        "labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
        "              ''.format(average_precision[\"micro\"]))\n",
        "\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
        "    lines.append(l)\n",
        "    labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n",
        "                  ''.format(i, average_precision[i]))\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.subplots_adjust(bottom=0.25)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Extension of Precision-Recall curve to multi-class')\n",
        "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}